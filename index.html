<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics">
  <meta name="keywords" content="3D talking head generation, Cross-modal generation">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
<!--  <link rel="icon" href="./static/images/favicon.svg">-->

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">

        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics</h1>

          <div class="is-size-4 publication-authors">
            <span class="author-block"><b>CVPR 2025 (Highlight)</b></span>
          </div>


          <div class="is-size-7 publication-authors">
            <span class="author-block"></span>
          </div>


          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://chae0382.github.io/">Lee Chae-Yeon</a><sup>1*</sup></sup>,</span>
            <span class="author-block">
              <a href="https://hyunbin70.github.io/">Oh Hyun-Bin</a><sup>1*</sup></sup>,</span>
            <span class="author-block">
              <a href="https://winterbloooom.github.io/profile/">Han EunGi</a><sup>1</sup></sup>,</span>
            <span class="author-block">
              <a href="https://sites.google.com/view/kimsungbin">Kim Sung-Bin</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://suekyeongnam.github.io/">Suekyeong Nam</a><sup>2</sup></sup>,</span>
            <span class="author-block">
              <a href="https://ami.kaist.ac.kr/members/tae-hyun-oh">Tae-Hyun Oh</a><sup>3</sup>
          </div>

          <div class="is-size-6 publication-authors">
            <span class="author-block"><sup>*</sup>denotes equal contribution</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>POSTECH,&nbsp</span>
            <span class="author-block"><sup>2</sup>KRAFTON,&nbsp</span>
            <span class="author-block"><sup>3</sup>KAIST</span>
          </div>


          <div class="column has-text-centered">
            <div class="publication-links">
              <span class="link-block">
                <a href="./media/perceptual-3d-talking-head/perceptual-3d-talking-head.pdf"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://arxiv.org/abs/2503.20308" class="external-link button is-normal is-rounded is-dark">
<!--                <a class="external-link button is-normal is-rounded is-dark">-->
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span>
              <span class="link-block">
                <a href="https://github.com/kaist-ami/Perceptual-3D-Talking-Head"
                class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                  </a>
              </span>
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="abstract section">
  <div class="container is-max-desktop">
    <!-- Abstract. -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h3 class="title is-3">Abstract</h3>
        <div class="content has-text-justified">
          <p>
            Recent advancements in speech-driven 3D talking head generation have made significant progress in lip synchronization.
            However, existing models still struggle to capture the perceptual alignment between varying speech characteristics and corresponding lip movements. 
            In this work, we claim that three criteria—Temporal Synchronization, Lip Readability, and Expressiveness—are crucial for achieving perceptually accurate lip movements. 
            Motivated by our hypothesis that a desirable representation space exists to meet these three criteria, we introduce a speech-mesh synchronized representation that captures intricate correspondences between speech signals and 3D face meshes. 
            We found that our learned representation exhibits desirable characteristics, and we plug it into existing models as a perceptual loss to better align lip movements to the given speech. 
            In addition, we utilize this representation as a perceptual metric and introduce two other physically grounded lip synchronization metrics to assess how well the generated 3D talking heads align with these three criteria. 
            Experiments show that training 3D talking head generation models with our perceptual loss significantly improve all three aspects of perceptually accurate lip synchronization.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Perceptually Accurate 3D Talking Head Generation</h2>
        <img src="./static/images/teaser_camready.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="max-width: 100%;"
      />
        <div class="content has-text-justified">
        <b>What defines perceptually accurate lip movement for a speech signal?</b> In this work, we define three criteria to assess perceptual alignment between speech and lip movements of 3D talking heads: Temporal Synchronization, Lip Readability, and Expressiveness (a). 
        The motivational hypothesis is the existence of a desirable representation space that models and complies well with the three criteria between diverse speech characteristics and 3D facial movements, as illustrated in (b); 
        where representations with the same phonemes are clustered, are sensitive to temporal synchronization, and follow a certain pattern as the speech intensity increases. 
        Consequently, we build a rich speech-mesh synchronized representation space that exhibits the desirable properties.
        </div>
      </div>
  </div>
</section>
  
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Human Studies on Alignment Criteria</h2>
    <div class="has-text-justified">
      <p>
        We present intriguing findings on human audio-visual perception through human studies.
      </p>
    </div>
    <br>
    
    <div class="columns is-centered">

      
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <img src="./static/images/human_study1.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="max-width: 70%;"
          />
          <div class="has-text-justified">
          <p>
            <b>Preference scores (1-3) for 3D talking heads with varying lip movement intensities paired with different speech intensities.</b>
            Humans prefer the lip movements with the intensity that match the intensity of speech.
          </p>
          </div>
          
          
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/human_study2.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="max-width: 70%;"
          />
            <div class="has-text-justified">
            <p>
              <b>Human preference between (A) samples with precise timing but low expressiveness, and (B) samples with high expressiveness but 100ms asynchrony—twice the commonly accepted 50ms threshold.</b>
              Humans are more sensitive to expressiveness than temporal synchronization when perceiving, highlighting the importance of expressiveness.
            </p>
            </div>
            
          </div>

        </div>
      </div>
    </div>
  </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Speech-Mesh Representation as Perceptual Loss</h2>
        <img src="./static/images/method.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="max-width: 100%;"
      />
        <div class="content has-text-justified">
        We train our speech-mesh representation space in a two-stage manner. 
        As a key application of our speech-mesh representation space, we propose a plug-in perceptual loss to 3D talking head models to enhance the quality of lip movements.
        </div>
      </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
    <h2 class="title is-3">Emergent desirable properties in our representation</h2>
    <div class="has-text-justified">
      <p>
        We found that our learned representation exhibits desirable characteristics we pursue for the three criteria.
      </p>
    </div>
    <br>
    
    <div class="columns is-centered">

      
      <!-- Visual Effects. -->
      <div class="column">
        <div class="content">
          <img src="./static/images/temporal_synchronization.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="max-width: 70%;"
          />
          <div class="has-text-justified">
          <p>
            <b>Temporal Synchronization.</b>
            Cosine similarity drops when temporal misalignment is introduced between input speech and 3D face mesh.
          </p>
          </div>
          
          
        </div>
      </div>
      <!--/ Visual Effects. -->

      <!-- Matting. -->
      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/lip_readability.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="max-width: 70%;"
          />
            <div class="has-text-justified">
            <p>
              <b>Lip Readability.</b>
              Our representation tends to form distinct clusters according to phonemes, with vowels and consonants grouped closely. 
              We also observe a directional progression in the feature space, shifting from phonemes with mouth opening (e.g., /aj/) to those with mouth closing (e.g., /f/).
            </p>
            </div>
            
          </div>

        </div>
      </div>

      <div class="column">
        <div class="columns is-centered">
          <div class="column content">
            <img src="./static/images/expressiveness.png"
                 class="interpolation-image"
                 alt="Interpolate start reference image."
                 style="max-width: 70%;"
          />
            <div class="has-text-justified">
            <p>
              <b>Expressiveness.</b>
              We plot speech features at varying speech intensities, showing a directional trend as intensity increases from lowest to highest.
            </p>
            </div>
            
          </div>

        </div>
      </div>
      
    </div>
  </div>
  </div>
</section>
  
<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Effectiveness of our perceptual loss for lip readability</h2>
        <video id="test_time" muted controls autoplay width="100%" height="100%" loop controlsList="nodownload">
        <source src="./media/perceptual-3d-talking-head/lip_readability.mp4" type="video/mp4" />
        </video>
      </div>
  </div>
</section>

<section class="hero is-small">
  <div class="hero-body">
    <div class="container is-max-desktop has-text-centered">
        <h2 class="title is-3">Effectiveness of our perceptual loss for expressiveness</h2>
        <video id="test_time" muted controls autoplay width="100%" height="100%" loop controlsList="nodownload">
        <source src="./media/perceptual-3d-talking-head/expressiveness.mp4" type="video/mp4" />
        </video>
      </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>
    @article{chae2025perceptually,
        title={Perceptually Accurate 3D Talking Head Generation: New Definitions, Speech-Mesh Representation, and Evaluation Metrics},
        author={Chae-Yeon, Lee and Hyun-Bin, Oh and EunGi, Han and Sung-Bin, Kim and Nam, Suekyeong and Oh, Tae-Hyun},
        journal={arXiv preprint arXiv:2503.20308},
        year={2025}
    }
    </code></pre>
  </div>
</section>
  
<section class="section" id="Acknowledgment">
  <div class="container is-max-desktop content">
    <h2 class="title">Acknowledgments</h2>
    <p>
      We thank the members of AMILab for their helpful discussions and proofreading. This research was supported by a grant from KRAFTON AI, and was also partially supported by Institute of Information & communications Technology Planning & Evaluation (IITP) grant funded by the Korea government (MSIT) (No.RS-2021-II212068, Artificial Intelligence Innovation Hub; No.RS-2023-00225630, Development of Artificial Intelligence for Text-based 3D Movie Generation; No. RS-2024-00457882, National AI Research Lab Project) and Culture, Sports and Tourism R&D Program through the Korea Creative Content Agency grant funded by the Ministry of Culture, Sports and Tourism in 2024 (Project Name: Development of barrier-free experiential XR contents technology to improve accessibility to online activities for the physically disabled, Project Number: RS-2024-00396700, Contribution Rate: 25%).
    </p>
  </div>
</section>
  
<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <a class="icon-link"
         href="./static/videos/nerfies_paper.pdf">
        <i class="fas fa-file-pdf"></i>
      </a>
<!--      <a class="icon-link" href="https://github.com/keunhong" class="external-link" disabled>-->
            <a class="icon-link" class="external-link" disabled>
        <i class="fab fa-github"></i>
      </a>
    </div>
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            This website is licensed under a <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
            <br>
              Source code mainly borrowed from <a
              href="https://github.com/nerfies/nerfies.github.io">Nerfies</a> website.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
